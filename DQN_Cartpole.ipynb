{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam\n",
    "from statistics import mean  \n",
    "    \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "\n",
    "# Solution criteria\n",
    "CONSECUTIVE_EPISODES_TO_SOLVE = 100  # Minimum number of consecutive episodes before eligible for solve\n",
    "MEAN_SCORE_TO_SOLVE = 200            # Average score required for solve\n",
    "  \n",
    "# Deep Q Network Class  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        # Initialize the action space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # Create a deque container for the neural net's training memory (experience replay)\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        # The Neural Network Model\n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    # Adds state, action, reward, next_state, and terminal to the DQN's memory\n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    # Determines whether the agent Explores or Exploits, then chooses an action\n",
    "    def act(self, state):  \n",
    "        \n",
    "        # Explore? (Exloration throttled by exploration_rate)\n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        # If not Exploring, then Exploit\n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    # Train the model using replay memory\n",
    "    def experience_replay(self):  \n",
    "        \n",
    "        # Check to see if number of entries in the memory container has reached our BATCH_SIZE constant\n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        \n",
    "        # If len(memory) >= BATCH_SIZE, then take a random sampling from memory of size BATCH_SIZE\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        \n",
    "        # Loop through each memory in the batch\n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            \n",
    "            # Assign the reward from the memory to a q_update variable\n",
    "            q_update = reward  \n",
    "            \n",
    "            # If the memory did not end in a terminal state\n",
    "            if not terminal:  \n",
    "                \n",
    "                # Calculate the updated Q value for the state\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "                \n",
    "            # Act and assign the result to the q_values table\n",
    "            q_values = self.model.predict(state)  \n",
    "            \n",
    "            # Update the value in the q_values table for the given action with the updated q value\n",
    "            q_values[0][action] = q_update  \n",
    "            \n",
    "            # Train the model based on the state and the q_values table\n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "            \n",
    "        # Adjust the exploration rate based on our EXPLORATION_DECAY constant\n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "# Function used to run the DQN in the cartpole environment  \n",
    "def cartpole():  \n",
    "    \n",
    "    # Initialize the environment and the DQN\n",
    "    env = gym.make(ENV_NAME)      \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    episode_counter = 0  # variable to track total number of episodes\n",
    "    \n",
    "    # Create a deque container to hold scores with a designated maxlen\n",
    "    scores = deque(maxlen=CONSECUTIVE_EPISODES_TO_SOLVE)\n",
    "    \n",
    "    # Main Loop variable\n",
    "    is_running = True\n",
    "    \n",
    "    # Main Loop\n",
    "    while is_running:  \n",
    "        \n",
    "        # increment the episode_counter\n",
    "        episode_counter += 1  \n",
    "        \n",
    "        # reset the environment\n",
    "        state = env.reset()  \n",
    "        \n",
    "        # Determine the starting state from the observation space\n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        \n",
    "        # Initialize step_counter\n",
    "        step_counter = 0  # variable to track number of steps before failing during the current episode\n",
    "        \n",
    "        # Episode Loop\n",
    "        while True:  \n",
    "            \n",
    "            # increment the step counter\n",
    "            step_counter += 1  \n",
    "            \n",
    "            # Render the environment (separate window)\n",
    "            env.render()  \n",
    "            \n",
    "            # Determine next action\n",
    "            action = dqn_solver.act(state)  \n",
    "            \n",
    "            # Act\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            # Calculate the reward for the action just taken\n",
    "            reward = reward if not terminal else -reward\n",
    "            \n",
    "            # Determine the next state from observation space\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            \n",
    "            # Remember the data related to the step just taken\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            \n",
    "            # Update the state variable to the next state\n",
    "            state = state_next  \n",
    "            \n",
    "            # Check to see if reached a terminal state for this episode\n",
    "            if terminal:  \n",
    "                \n",
    "                print(\"Episode: \" + str(episode_counter))\n",
    "                print(\"Exploration Rate: \" + str(dqn_solver.exploration_rate) + \", Episode Score: \" + str(step_counter))  \n",
    "                                \n",
    "                # Append the score for this episode to the scores deque (score = number of steps)\n",
    "                scores.append(step_counter)\n",
    "                \n",
    "                # Calculate the mean score for all episodes in the scores deque\n",
    "                mean_score = mean(scores)\n",
    "                print(\"Mean score: \" + str(mean_score) + \"\\n\")\n",
    "                \n",
    "                # Check to see if the agent solved the cartpole problem according to our criteria constants\n",
    "                if mean_score >= MEAN_SCORE_TO_SOLVE and len(scores) >= CONSECUTIVE_EPISODES_TO_SOLVE:\n",
    "                    print(\"CONGRATULATIONS!\")\n",
    "                    print(\"Solved after \" + str(episode_counter) + \" episodes.\")\n",
    "                    \n",
    "                    # End the loop\n",
    "                    is_running = False\n",
    "                    \n",
    "                    # Close the environment\n",
    "                    env.close()\n",
    "                \n",
    "                # Start a new episode\n",
    "                break  \n",
    "            \n",
    "            # Use experience replay to train the model\n",
    "            dqn_solver.experience_replay()  \n",
    "            \n",
    "# Start the DQN agent working through the cartpole problem\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
